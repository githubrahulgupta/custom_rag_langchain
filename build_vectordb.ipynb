{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config - INFO - Current Working Directory: c:\\Users\\Rahul Gupta\\Documents\\RG\\GenAI\\0.self_explore\\custom_rag\n",
      "config - INFO - Folder separator used w.r.t OS: \\\n",
      "config - INFO - Log File: c:\\Users\\Rahul Gupta\\Documents\\RG\\GenAI\\0.self_explore\\custom_rag\\logs\\coe_demo_2024_05_18_18_33_20.log\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "import ast, array, shutil, sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()\n",
    "logger.debug(f'Environment file loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config - INFO - Documents Directory: c:\\Users\\Rahul Gupta\\Documents\\RG\\GenAI\\0.self_explore\\custom_rag\\documents\n",
      "config - INFO - Documents folder being read: c:\\Users\\Rahul Gupta\\Documents\\RG\\GenAI\\0.self_explore\\custom_rag\\documents\\coe_demo\n"
     ]
    }
   ],
   "source": [
    "from load_and_chunk import get_docs_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Using OCI vectordb \n",
    "import oracledb\n",
    "\n",
    "# Luigi's OracleVectorStore wrapper for LangChain\n",
    "from oracle_vector_db_lc import OracleVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# required for oracle vectordb\n",
    "# \n",
    "def initialize_vectordb_tables(cursor):\n",
    "    logger.debug(f'#### ENTER initialize_vectordb_tables() function ####')\n",
    "    # Drop tables\n",
    "    table = f'{VECTORDB_FOLDER}_CHUNKS'\n",
    "    # print(f'table: {table}')\n",
    "    query = f\"\"\"\n",
    "    begin\n",
    "        execute immediate 'drop table {table}';\n",
    "        exception when others then if sqlcode <> -942 then raise; end if;\n",
    "    end;\"\"\"\n",
    "    # print(f'query: {query}')\n",
    "    cursor.execute(query)\n",
    "    \n",
    "    table = f'{VECTORDB_FOLDER}_DOCS'\n",
    "    # print(f'table: {table}')\n",
    "    query = f\"\"\"\n",
    "    begin\n",
    "        execute immediate 'drop table {table}';\n",
    "        exception when others then if sqlcode <> -942 then raise; end if;\n",
    "    end;\"\"\"\n",
    "    # print(f'query: {query}')\n",
    "    cursor.execute(query)\n",
    "    \n",
    "    table = f'{VECTORDB_FOLDER}_VECTORS'\n",
    "    # print(f'table: {table}')\n",
    "    query = f\"\"\"\n",
    "    begin\n",
    "        execute immediate 'drop table {table}';\n",
    "        exception when others then if sqlcode <> -942 then raise; end if;\n",
    "    end;\"\"\"\n",
    "    # print(f'query: {query}')\n",
    "    cursor.execute(query)\n",
    "\n",
    "    logger.info(f\"\\nAll {VECTORDB_FOLDER} tables in DATABASE SCHEMA {os.getenv('DB_USER')}: \")\n",
    "    query = f\"\"\"SELECT table_name FROM all_tables WHERE owner = '{os.getenv('DB_USER')}' and table_name like '{VECTORDB_FOLDER}%'\"\"\"\n",
    "    # print(f'all tables: {query}')\n",
    "    cursor.execute(query)\n",
    "    \n",
    "    for row in cursor:\n",
    "         logger.info(row)\n",
    "    \n",
    "    # create tables\n",
    "    query = f\"\"\"\n",
    "    create table {VECTORDB_FOLDER}_VECTORS (\n",
    "        id VARCHAR2(64) NOT NULL,\n",
    "        VEC VECTOR(1024, FLOAT64),\n",
    "        primary key (id))\"\"\"\n",
    "    # print(f'query: {query}')\n",
    "    cursor.execute(query)\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    create table {VECTORDB_FOLDER}_DOCS (\n",
    "        ID NUMBER NOT NULL,\n",
    "        NAME VARCHAR2(100) NOT NULL,\n",
    "        PRIMARY KEY (ID)  )\"\"\"\n",
    "    # print(f'query: {query}')\n",
    "    cursor.execute(query)\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    create table {VECTORDB_FOLDER}_CHUNKS \n",
    "        (ID VARCHAR2(64) NOT NULL,\n",
    "        CHUNK CLOB,\n",
    "        PAGE_NUM VARCHAR2(10),\n",
    "        DOC_ID NUMBER,\n",
    "        PRIMARY KEY (\"ID\"),\n",
    "        CONSTRAINT fk_{VECTORDB_FOLDER}_doc\n",
    "                FOREIGN KEY (DOC_ID)\n",
    "                REFERENCES {VECTORDB_FOLDER}_DOCS (ID)\n",
    "        )\"\"\"\n",
    "    # print(f'query: {query}')\n",
    "    cursor.execute(query)\n",
    "    \n",
    "    logger.info(\"Oracle vectordb tables initialized...\")\n",
    "    logger.debug(f'#### EXIT initialize_vectordb_tables() function ####')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# required for oracle vectordb\n",
    "# with this function every book added to DB is registered with a unique id\n",
    "# \n",
    "def register_docs(docs_name, connection):\n",
    "    logger.debug(f'#### ENTER register_docs() function ####')\n",
    "    logger.info(f\"Registering documents to vectordb table {VECTORDB_FOLDER}_DOCS...\")\n",
    "    with connection.cursor() as cursor:\n",
    "                \n",
    "        # get the new key\n",
    "        query = f\"SELECT MAX(ID) FROM {VECTORDB_FOLDER}_DOCS\"\n",
    "        cursor.execute(query)\n",
    "\n",
    "        # Fetch the result\n",
    "        row = cursor.fetchone()\n",
    "\n",
    "        if row[0] is not None:\n",
    "            new_key = row[0] + 1\n",
    "        else:\n",
    "            new_key = 1\n",
    "\n",
    "        # insert the record for the book\n",
    "        query = f\"INSERT INTO {VECTORDB_FOLDER}_DOCS (ID, NAME) VALUES (:1, :2)\"\n",
    "\n",
    "        # Execute the query with your values\n",
    "        cursor.execute(query, [new_key, docs_name])\n",
    "\n",
    "    logger.info(f\"Completed registering documents inside Oracle vectordb\")\n",
    "    logger.debug(f'#### EXIT register_docs() function ####')\n",
    "    return new_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# required for oracle vectordb\n",
    "# \n",
    "def save_chunks_in_db(document_chunks, doc_id, docs_name, connection):\n",
    "    logger.debug(f'#### ENTER save_chunks_in_db() function ####')\n",
    "    tot_errors = 0\n",
    "    \n",
    "    chunk_id = [] \n",
    "    chunk_text = [] \n",
    "    \n",
    "    document_splits_str = [str(item) for item in document_chunks]\n",
    "    \n",
    "    with connection.cursor() as cursor:\n",
    "        logger.info(\"Saving chunks to DB...\")\n",
    "        cursor.setinputsizes(None, oracledb.DB_TYPE_CLOB)\n",
    "\n",
    "        for i, chunk in enumerate(document_splits_str):\n",
    "            # chunk_id = i+1\n",
    "            chunk_id.append(i+1)\n",
    "\n",
    "            chunk_text_start = chunk.find(\"page_content=\")\n",
    "            chunk_metadata_start = chunk.find(\"metadata=\")\n",
    "\n",
    "            chunk_content = chunk[chunk_text_start+13:chunk_metadata_start]\n",
    "            chunk_text.append(chunk_content)\n",
    "            \n",
    "            chunk_metadata=chunk[chunk_metadata_start+9:]\n",
    "            chunk_metadata = ast.literal_eval(chunk_metadata) # parses the input string as a Python literal structure, such as a string, list, tuple, or dictionary.\n",
    "\n",
    "            chunk_doc_id = doc_id[docs_name.index(chunk_metadata[\"name\"])]\n",
    "            chunk_page_num = int(chunk_metadata[\"page\"])+1\n",
    "            \n",
    "            try:\n",
    "                query = f\"insert into {VECTORDB_FOLDER}_CHUNKS (ID, CHUNK, PAGE_NUM, DOC_ID) values (:1, :2, :3, :4)\"\n",
    "                cursor.execute(query, [i+1, chunk_content, chunk_page_num, chunk_doc_id])\n",
    "            except Exception as e:\n",
    "                logger.exception(\"Exception occured in save chunks...\")\n",
    "                tot_errors += 1\n",
    "            \n",
    "        logger.info(f'No. of chunk ids created inside get_chunk_content(): {len(chunk_id)}')\n",
    "        logger.info(f\"Completed savings chunks inside oracle vectordb\")\n",
    "        logger.debug(f'#### EXIT save_chunks_in_db() function ####')\n",
    "        return chunk_id, chunk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# required for oracle vectordb\n",
    "# \n",
    "def save_embeddings_in_db(embeddings, chunks_id, connection):\n",
    "    logger.debug(f'#### ENTER save_embeddings_in_db() function ####')\n",
    "    tot_errors = 0\n",
    "\n",
    "    with connection.cursor() as cursor:\n",
    "        logger.info(\"Saving embeddings to DB...\")\n",
    "\n",
    "        for id, vector in zip(chunks_id, embeddings):        \n",
    "            # 'f' single precision 'd' double precision\n",
    "            if EMBEDDINGS_BITS == 64:\n",
    "                input_array = array.array(\"d\", vector)\n",
    "            else:\n",
    "                # 32 bits\n",
    "                input_array = array.array(\"f\", vector)\n",
    "\n",
    "            try:\n",
    "                # insert single embedding\n",
    "                query = f\"insert into {VECTORDB_FOLDER}_VECTORS values (:1, :2)\"\n",
    "                cursor.execute(query, [id, input_array])\n",
    "            except Exception as e:\n",
    "                logger.exception(\"Error in save embeddings...\")\n",
    "                tot_errors += 1\n",
    "\n",
    "    logger.info(f\"Total no. of errors in save_embeddings: {tot_errors}\")\n",
    "    logger.info(f\"Completed savings embeddings inside oracle vectordb\")\n",
    "    logger.debug(f'#### EXIT save_embeddings_in_db() function ####')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# create vector store\n",
    "#\n",
    "def create_vectordb(VECTOR_STORE, document_chunks, embedder):\n",
    "    logger.debug(f'#### ENTER create_vectordb() function ####')\n",
    "    logger.info(f\"Using {VECTOR_STORE} as Vector Store...\")\n",
    "\n",
    "    if VECTOR_STORE == \"CHROMA\":\n",
    "        \n",
    "        # in-memory chromadb\n",
    "        if VECTOR_STORE_TYPE == 'in-memory':\n",
    "            logger.info(f'{VECTOR_STORE_TYPE} vector store being created')\n",
    "            try:\n",
    "                vectorstore = Chroma.from_documents(\n",
    "                    documents=document_chunks, \n",
    "                    embedding=embedder\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.exception(\"Exception occurred\")\n",
    "                sys.exit(1) # to exit the program with a non-zero exit code, indicating that an error occurred\n",
    "\n",
    "        # persistant chromadb\n",
    "        elif VECTOR_STORE_TYPE == 'persistent':\n",
    "            logger.info(f'{VECTOR_STORE_TYPE} vector store being created')\n",
    "    #         vectorstore = Chroma.from_documents(\n",
    "    #             documents=document_chunks, \n",
    "    #             embedding=embedder, \n",
    "    #             persist_directory=\"./chroma_db\"\n",
    "    #         )\n",
    "        \n",
    "            # OCI GenAI Cohere Embedding supports a size of [1:96] as input array\n",
    "            # Below code takes into consideration input array size limit\n",
    "            vectordb_path = f\"./vectorstore/{VECTORDB_FOLDER}_chromadb/\" \n",
    "            if os.path.isdir(vectordb_path):\n",
    "                shutil.rmtree(vectordb_path) \n",
    "                logger.info(f'Directory called {VECTORDB_FOLDER}_chromadb has been deleted from vectorstore folder')\n",
    "\n",
    "            logger.info(f'A new directory called {VECTORDB_FOLDER}_chromadb will be created under vectorstore folder')            \n",
    "            vectorstore = Chroma(\n",
    "                            persist_directory=vectordb_path,\n",
    "                            embedding_function=embedder)\n",
    "            \n",
    "            ids_list = [str(pos+1) for pos, s in enumerate(document_chunks)]\n",
    "            logger.info(f'No. of document splits: {len(ids_list)}')\n",
    "            \n",
    "            logger.info(f'Document embeddings being created in a batch size of {EMBED_BATCH_SIZE} docs at a time')\n",
    "\n",
    "            start=0\n",
    "            while start < len(document_chunks):\n",
    "                try:\n",
    "                    vectorstore.add_documents(\n",
    "                    ids = ids_list[start:start+EMBED_BATCH_SIZE],\n",
    "                    documents = document_chunks[start:start+EMBED_BATCH_SIZE]\n",
    "                    )\n",
    "                    start+=EMBED_BATCH_SIZE\n",
    "                except Exception as e:\n",
    "                    logger.exception('Exception occured')\n",
    "                    # print(f'\\nERROR OCCURRED WHILE CREATING VECTOR DB:\\n {error} ')\n",
    "                    logger.debug(f'\\nStart = {start}, End = {start+EMBED_BATCH_SIZE}')\n",
    "                    for i, item in enumerate(document_chunks[start:start+EMBED_BATCH_SIZE]):\n",
    "                        logger.debug(f'ID# : {ids_list[start+i-1]}')\n",
    "                        logger.debug(f'Document: {item}')\n",
    "                    # break\n",
    "                    sys.exit(1) # to exit the program with a non-zero exit code, indicating that an error occurred\n",
    "            \n",
    "            # restore persistant chromadb\n",
    "            vectorstore = Chroma(persist_directory=f\"./vectorstore/{VECTORDB_FOLDER}_chromadb\", embedding_function=embedder)\n",
    "        \n",
    "    elif VECTOR_STORE == \"FAISS\":\n",
    "        try:\n",
    "            vectorstore = FAISS.from_documents(\n",
    "                documents=document_chunks, \n",
    "                embedding=embedder\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Exception occured\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "    elif VECTOR_STORE == \"ORACLE\":\n",
    "        # connect to db\n",
    "        logger.info(\"Connecting to Oracle DB...\")\n",
    "\n",
    "        DSN = os.getenv('DB_HOST_IP') + \"/\" + os.getenv('DB_SERVICE')\n",
    "\n",
    "        with oracledb.connect(user=os.getenv('DB_USER'), password=os.getenv('DB_PWD'), dsn=DSN) as connection:\n",
    "            logger.info(\"Successfully connected to Oracle Database...\")\n",
    "            \n",
    "            initialize_vectordb_tables(connection.cursor())\n",
    "            \n",
    "            # determine doc_id and save in table {VECTORDB_FOLDER}_DOCS\n",
    "            docs_name = get_docs_name()\n",
    "            doc_id = [register_docs(doc, connection) for doc in docs_name]\n",
    "            # book_id = register_book(docs_name, connection)\n",
    "\n",
    "            chunk_id, chunk_text = save_chunks_in_db(document_chunks, doc_id, docs_name, connection)\n",
    "            \n",
    "            logger.info(f'Document embeddings being created in a batch size of {EMBED_BATCH_SIZE} docs at a time')\n",
    "\n",
    "            start=0\n",
    "            embeddings = []\n",
    "            while start < len(document_chunks):\n",
    "                try:\n",
    "                    chunk_embeddings = embedder.embed_documents(chunk_text[start:start+EMBED_BATCH_SIZE])\n",
    "                    embeddings.extend(chunk_embeddings)\n",
    "                    start+=EMBED_BATCH_SIZE\n",
    "                except Exception as e:\n",
    "                    logger.exception(f'\\nERROR OCCURRED WHILE CREATING ORACLE VECTOR DB')\n",
    "                    logger.debug(f'\\nStart = {start}, End = {start+EMBED_BATCH_SIZE}')\n",
    "                    for i, item in enumerate(document_chunks[start:start+EMBED_BATCH_SIZE]):\n",
    "                        logger.debug(f'ID# : {ids_list[start+i-1]}')\n",
    "                        logger.debug(f'Document: {item}')\n",
    "                    # break\n",
    "                    sys.exit(1) # to exit the program with a non-zero exit code, indicating that an error occurred\n",
    "            logger.info(f'Number of embeddings created: {len(embeddings)}')\n",
    "\n",
    "            # store embeddings\n",
    "            # here we save in DB\n",
    "            save_embeddings_in_db(embeddings, chunk_id, connection)\n",
    "\n",
    "            # a txn is a document\n",
    "            connection.commit()       \n",
    "\n",
    "        # restore oracle vectordb\n",
    "        # OracleVectorStore is custom class that inherits from langchain_core.vectorstores\n",
    "        vectorstore = OracleVectorStore(embedding_function=embedder.embed_query, verbose=True)\n",
    "    logger.debug(f'#### EXIT create_vectordb() function ####')\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# create retrievere with optional reranker\n",
    "#\n",
    "def create_retriever(vectorstore):\n",
    "    logger.debug(f'#### ENTER create_retriever() function ####')\n",
    "    if ADD_RERANKER == False:\n",
    "        # no reranking\n",
    "        logger.info(f\"No reranking...\")\n",
    "        try:\n",
    "            retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Exception occured\")\n",
    "            sys.exit(1)\n",
    "        # print output here\n",
    "        \n",
    "    else:\n",
    "        # to add reranking\n",
    "        logger.info(\"Adding reranking to QA chain...\")\n",
    "\n",
    "        # compressor = CohereRerank(cohere_api_key=os.getenv('COHERE_API_KEY'))\n",
    "\n",
    "        # base_retriever = vectorstore.as_retriever(\n",
    "        #     search_kwargs={\"k\": top_k}\n",
    "        # )\n",
    "\n",
    "        # retriever = ContextualCompressionRetriever(\n",
    "        #     base_compressor=compressor, base_retriever=base_retriever\n",
    "        # )\n",
    "    logger.debug(f'#### EXIT create_retriever() function ####')\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load a list of pdf documents\n",
    "# all_docs = load_all_docs()\n",
    "# all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Split pages in chunks\n",
    "# document_chunks = split_in_chunks(all_docs)\n",
    "# document_chunks\n",
    "# doc_list, metadata_list = docs_and_metadata(document_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config - INFO - Loading OCI GenAI Cohere Embeddings Model: cohere.embed-english-v3.0\n"
     ]
    }
   ],
   "source": [
    "# 3. Load embeddings model\n",
    "from build_embedder import *\n",
    "embedder = create_embedder()\n",
    "# embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create a Vectore Store and store embeddings\n",
    "# vectorstore = create_vectordb(VECTOR_STORE, document_chunks, embedder)\n",
    "# vectorstore = Chroma(persist_directory=f\"./vectorstore/{VECTORDB_FOLDER}_chromadb\", embedding_function=embedder)\n",
    "# vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config - INFO - No reranking...\n",
      "c:\\Users\\Rahul Gupta\\anaconda3\\envs\\condarag\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Lifecycle of machine learning models', metadata={'name': 'data-science-lifecycle-ebook.pdf', 'page': 0, 'source': 'C:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag\\\\documents\\\\coe_demo\\\\\\\\data-science-lifecycle-ebook.pdf'}),\n",
       " Document(page_content='and make decisions about any necessary data 4 II. Data preparation and exploration Source Scikit Learn Library https scikit learn.org Visualization performed with Oracle Cloud Infrastructure Data Science https docs.cloud.oracle.com en us iaas data science using data science.htm After getting the data data scientists have to', metadata={'name': 'data-science-lifecycle-ebook.pdf', 'page': 3, 'source': 'C:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag\\\\documents\\\\coe_demo\\\\\\\\data-science-lifecycle-ebook.pdf'}),\n",
       " Document(page_content='accessing the data. Typically data scientists will obtain the data for the business problems they are working on by querying the databases where their companies store their data. In addition there is a lot of value in unstructured datasets that do not fit well into a relational database e.g. logs raw texts images videos etc. . These datasets are heavily processed via Extract Transform Load ETL pipelines written by data engineers and data scientists. These datasets either reside in a data lake or in a database either relational or not . When data scientists do not have the data needed to solve their problems they can get the data by scraping data from websites purchasing data from data providers or collecting the data from surveys clickstream data sensors cameras etc.', metadata={'name': 'data-science-lifecycle-ebook.pdf', 'page': 2, 'source': 'C:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\custom_rag\\\\documents\\\\coe_demo\\\\\\\\data-science-lifecycle-ebook.pdf'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorstore.similarity_search('what is data science?')\n",
    "\n",
    "# retreiever = create_retriever(vectorstore)\n",
    "# retreiever.get_relevant_documents('what is data science?') # TO BE TESTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config - INFO - #### ENTER release_log_file() function ####\n",
      "config - INFO - Releasing log file...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file released successfully.\n"
     ]
    }
   ],
   "source": [
    "# release_log_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condarag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
