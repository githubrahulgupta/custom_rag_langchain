{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config - INFO - Current Working Directory: c:\\Users\\Rahul Gupta\\Documents\\RG\\GenAI\\0.self_explore\n",
      "config - INFO - Folder separator used w.r.t OS: \\\n",
      "config - INFO - Log File: c:\\Users\\Rahul Gupta\\Documents\\RG\\GenAI\\0.self_explore\\logs\\coe_demo_2024_05_18_12_43_57.log\n",
      "config - INFO - Documents Directory: c:\\Users\\Rahul Gupta\\Documents\\RG\\GenAI\\0.self_explore\\documents\n",
      "config - INFO - Documents folder being read: c:\\Users\\Rahul Gupta\\Documents\\RG\\GenAI\\0.self_explore\\documents\\coe_demo\n"
     ]
    }
   ],
   "source": [
    "from build_embedder import *\n",
    "from build_vectordb import create_retriever\n",
    "from build_llm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This one is to be used with Streamlit\n",
    "#\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# Luigi's OracleVectorStore wrapper for LangChain\n",
    "from oracle_vector_db_lc import OracleVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "my_conv_memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_response(my_dict):\n",
    "    desired_order = ['question', 'generated_question', 'answer', 'chat_history', 'source_documents']\n",
    "    for key in desired_order:\n",
    "        logger.info(f\"{key}:\")\n",
    "        # print(f\"    {type(my_dict[key])}\")\n",
    "        if isinstance(my_dict[key], list): \n",
    "            for item in my_dict[key]:\n",
    "                logger.info(f'{item}')\n",
    "        else:\n",
    "            logger.info(f\"    {my_dict[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# def: get_answer  from LLM\n",
    "#\n",
    "def get_answer(rag_chain, question):\n",
    "    response = rag_chain.invoke(question)\n",
    "\n",
    "    if DEBUG:\n",
    "        # logger.debug(f\"Question: {question}\")\n",
    "        # logger.debug(\"The response:\")\n",
    "        # logger.debug(response)\n",
    "        print_response(response)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Initialize_rag_chain\n",
    "#\n",
    "# to run it only once\n",
    "@st.cache_resource\n",
    "def create_rag_chain():\n",
    "    logger.debug(f'#### ENTER create_rag_chain() function ####')\n",
    "\n",
    "    # 1. Load embeddings model\n",
    "    embedder = create_embedder()\n",
    "\n",
    "    # 2. restore vectordb\n",
    "    if VECTOR_STORE == \"CHROMA\":\n",
    "        # restore persistant chromadb\n",
    "        try:\n",
    "            vectorstore = Chroma(persist_directory=f\"./vectorstore/{VECTORDB_FOLDER}_chromadb\", embedding_function=embedder)\n",
    "            logger.info(f'{VECTOR_STORE} vectordb restored')\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Exception occured\")\n",
    "            sys.exit(1)\n",
    "    elif VECTOR_STORE == \"ORACLE\":\n",
    "        # restore oracle vectordb\n",
    "        try:\n",
    "            vectorstore = OracleVectorStore(embedding_function=embedder.embed_query, verbose=True)\n",
    "            logger.info(f'{VECTOR_STORE} vectordb restored')\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Exception occured\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        print(f'Please check the vector store to be used as part of configuration')\n",
    "        exit()\n",
    "\n",
    "    # 3. Create a retriever\n",
    "    retriever = create_retriever(vectorstore)\n",
    "\n",
    "    # 4. Build the LLM\n",
    "    llm = build_llm()\n",
    "    \n",
    "    # 5. Build prompt template\n",
    "    template = \"\"\"You are a helpful assistant who searches for answer by going through provided context below. \\\n",
    "    Answer the question as truthfully as possible using only the context provided. \\\n",
    "    If the answer is not contained within the context below, say \"I don't know\". \\\n",
    "    Do not end your answer with a question.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    rag_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    # 6. Build conversation memory\n",
    "    logger.info(\"Initializing Conversation Buffer Memory\")\n",
    "    global my_conv_memory\n",
    "    my_conv_memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        input_key='question', output_key='answer',\n",
    "        return_messages=True,\n",
    "        verbose = True\n",
    "    )\n",
    "\n",
    "    # 7. Create a conversation chain\n",
    "    logger.info(\"Initializing Conversation Retrieval Chain\")\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm, \n",
    "        chain_type=\"stuff\", \n",
    "        retriever=retriever, \n",
    "        memory=my_conv_memory,\n",
    "        return_source_documents=True,\n",
    "        return_generated_question=True,\n",
    "        rephrase_question=False, \n",
    "        combine_docs_chain_kwargs={'prompt': rag_prompt}\n",
    "    )\n",
    "\n",
    "\n",
    "    logger.info(\"\\nRAG Chain created successfully\")\n",
    "\n",
    "    logger.debug(f'#### EXIT create_rag_chain() function ####')\n",
    "    return qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Reset Conversation Buffer Memory\n",
    "# \n",
    "def clear_conv_memory():\n",
    "    logger.debug(f'#### ENTER clear_conv_memory() function ####')\n",
    "    global my_conv_memory\n",
    "    my_conv_memory.clear()\n",
    "    logger.info(f'Conversation Memory cleared')\n",
    "    logger.debug(f'#### EXIT clear_conv_memory() function ####')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config - INFO - Documents being processed:\n",
      "config - INFO - (1) data-science-lifecycle-ebook.pdf\n",
      "config - INFO - (2) oracle-autonomous-database-technical-overview.pdf\n",
      "config - INFO - Loading document: data-science-lifecycle-ebook.pdf...\n",
      "config - INFO - Loading document: oracle-autonomous-database-technical-overview.pdf...\n",
      "config - INFO - \n",
      "Loaded 32 docs...\n",
      "\n",
      "config - INFO - Splitted the document in 145 chunks...\n",
      "config - INFO - Number of non-empty chunks: 145\n",
      "config - INFO - Loading OCI GenAI Cohere Embeddings Model: cohere.embed-english-v3.0\n",
      "config - INFO - Using CHROMA as Vector Store...\n",
      "config - INFO - persistent vector store being created\n",
      "config - INFO - Directory called coe_demo_chromadb has been deleted from vectorstore folder\n",
      "config - INFO - A new directory called coe_demo_chromadb will be created under vectorstore folder\n",
      "config - INFO - No. of document splits: 145\n",
      "config - INFO - Document embeddings being created in a batch size of 96 docs at a time\n",
      "config - INFO - No reranking...\n",
      "config - INFO - Using OCI_GEN_AI cohere.command model...\n",
      "config - INFO - Initializing Conversation Buffer Memory\n",
      "config - INFO - Initializing Conversation Retrieval Chain\n",
      "config - INFO - \n",
      "RAG Chain created successfully\n"
     ]
    }
   ],
   "source": [
    "# qa = create_rag_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = qa.invoke(\"what is machine learning?\")\n",
    "# print_response(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = qa.invoke(\"what are different steps needed to build it?\")\n",
    "# print_response(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = qa.invoke(\"what is oracle autonomous database?\")\n",
    "# print_response(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = qa.invoke(\"what are the different offerings that it provides?\")\n",
    "# print_response(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config - INFO - question:\n",
      "config - INFO -     Provide more details about Oracle Autonomous Data Warehouse\n",
      "config - INFO - generated_question:\n",
      "config - INFO -      What are the key features of the Oracle Autonomous Data Warehouse and how do they make it suitable for data warehousing, data marts, data lakes, and machine learning workloads?\n",
      "config - INFO - answer:\n",
      "config - INFO -     Oracle Autonomous Data Warehouse is a solution designed for specific data warehousing, data mart, and data lake business requirements. It is tailored to streamline machine learning and data warehousing operations by utilizing data modeling techniques such as star schema to ensure optimal data structures for business analysts and data scientists. Given the large volumes of data typically housed in data warehouses, Oracle ADW uses summary data representations and highly parallel SQL to provide quick responses times, enabling efficient data processing and streaming into the database. \n",
      "\n",
      "Oracle Autonomous Data Warehouse is built to be separate from transaction processing applications, which is typical of data warehouses that deal with specific business requirements. This separation caters to the specialized nature of data warehousing tasks, including data analysis, trend analysis, and reporting. \n",
      "\n",
      "The Oracle Autonomous Database enables the Autonomous Data Warehouse to be offered in two editions. These editions include the Oracle Autonomous Data Warehouse ADW, tailored for data warehousing, data mart, data lake, and machine learning workloads. \n",
      "\n",
      "Would you like to know more about Oracle Autonomous Data Warehouse or any of its features? \n",
      "config - INFO - chat_history:\n",
      "config - INFO - content='what is machine learning?'\n",
      "config - INFO - content='Machine learning is an area that is increasingly being invested in by enterprises because it allows them to leverage data to gain insights and make processes more efficient. \\n\\nWould you like to know more about the fields of machine learning? '\n",
      "config - INFO - content='what are different steps needed to build it?'\n",
      "config - INFO - content='There are six steps needed to build a machine learning model. They include data access and collection, data preparation and exploration, model build and training, model evaluation, model deployment, and model monitoring. It is important to remember that machine learning is an iterative process and the steps outlined above will be reiterated and improved upon many times. Would you like to know more about any of these steps specifically? '\n",
      "config - INFO - content='what is oracle autonomous database?'\n",
      "config - INFO - content='Oracle Autonomous Database is a cloud-based database management service that combines the flexibility of the cloud with the power of machine learning. Built over three decades by Oracle, Autonomous Database serves thousands of enterprise customers worldwide. It is designed to be self-driving, self-securing, and self-repairing, requiring minimal human maintenance and intervention. Machine Learning plays a crucial role in automating issue resolution within the database and helps to optimize performance. Autonomous Database is available both in the cloud and on-premises, offering flexibility to businesses and enterprises of all sizes. \\n\\nWould you like to know more about Oracle Autonomous Database? '\n",
      "config - INFO - content='what are the different offerings that it provides?'\n",
      "config - INFO - content='The Oracle Autonomous Database provides two offerings that are tailored to specific workloads:\\n\\n1. Oracle Autonomous Data Warehouse ADW is tailored for Data Warehousing, Data Marts, Data Lakes, and Machine Learning workloads.\\n2. Oracle Autonomous Transaction Processing ATP is tailored for Online Transaction Processing, Batch reporting, IoT application development, machine learning, and mixed workload environments.\\n\\nWould you like to know more about these offerings? '\n",
      "config - INFO - content='Provide more details about Oracle Autonomous Data Warehouse'\n",
      "config - INFO - content='Oracle Autonomous Data Warehouse is a solution designed for specific data warehousing, data mart, and data lake business requirements. It is tailored to streamline machine learning and data warehousing operations by utilizing data modeling techniques such as star schema to ensure optimal data structures for business analysts and data scientists. Given the large volumes of data typically housed in data warehouses, Oracle ADW uses summary data representations and highly parallel SQL to provide quick responses times, enabling efficient data processing and streaming into the database. \\n\\nOracle Autonomous Data Warehouse is built to be separate from transaction processing applications, which is typical of data warehouses that deal with specific business requirements. This separation caters to the specialized nature of data warehousing tasks, including data analysis, trend analysis, and reporting. \\n\\nThe Oracle Autonomous Database enables the Autonomous Data Warehouse to be offered in two editions. These editions include the Oracle Autonomous Data Warehouse ADW, tailored for data warehousing, data mart, data lake, and machine learning workloads. \\n\\nWould you like to know more about Oracle Autonomous Data Warehouse or any of its features? '\n",
      "config - INFO - source_documents:\n",
      "config - INFO - page_content='6 White Paper Oracle Autonomous Database Technical Overview Version 2. 2 Copyright 2023 Oracle and or its affiliates Public Autonomous Data Warehouse As the name implies Oracle Autonomous Data Warehouse ADW is tailored for Data Warehouse and related workloads including Data Marts Machine Learning or as part of a Data Lake deployment. These systems and databases are generally separated from Transaction Processin g applications and are constructed to meet specific business needs. Data Warehouses often use data modeling approaches such as Star Schema and other techniques to ensure data structures meet the needs of business users conducting data analysis and Data Sc ientists performing trend analysis. Data' metadata={'name': 'oracle-autonomous-database-technical-overview.pdf', 'page': 5, 'source': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\documents\\\\coe_demo\\\\\\\\oracle-autonomous-database-technical-overview.pdf'}\n",
      "config - INFO - page_content='Warehouses typically house large volumes of data that is processed in bulk or streamed into the database. Data Warehouses often rely on summary data representation and highly parallel SQL to provide fast response ti mes. Oracle Autonomous Data Warehouse is tailored specifically to these use cases. Autonomous Transaction Processing Oracle Autonomous Transaction Processing ATP brings the same autonomous capabilities found in ADW into the Transaction Processing and mi xed workload arena. ATP is tailored primarily for complex Transaction Processing workloads that include operational reporting and or batch data processing. The ability to run mixed workloads in a' metadata={'name': 'oracle-autonomous-database-technical-overview.pdf', 'page': 5, 'source': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\documents\\\\coe_demo\\\\\\\\oracle-autonomous-database-technical-overview.pdf'}\n",
      "config - INFO - page_content='that customers can use to implement their own solutions. AUTONOMOUS DATABASE SERVICES The underlying converged database capabilities of the Oracle Database enable the Autonomous Database to be offered in two editions that are specifically tailored to a workload following Oracle s Best Practice recommendations. Oracle Autonomous Data Warehouse ADW is tailored to Data Warehousing Data Marts Data Lakes and Machine Learning work loads. Oracle Autonomous Transaction Processing ATP is tailored to On Line Transaction Processing Batch reporting IoT application development machine learning and mixed workload environments.' metadata={'name': 'oracle-autonomous-database-technical-overview.pdf', 'page': 4, 'source': 'c:\\\\Users\\\\Rahul Gupta\\\\Documents\\\\RG\\\\GenAI\\\\0.self_explore\\\\documents\\\\coe_demo\\\\\\\\oracle-autonomous-database-technical-overview.pdf'}\n"
     ]
    }
   ],
   "source": [
    "# result = qa.invoke(\"Provide more details about Oracle Autonomous Data Warehouse\")\n",
    "# print_response(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condarag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
