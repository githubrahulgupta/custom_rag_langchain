from build_vectordb import *
from build_llm import *

from langchain.prompts import ChatPromptTemplate

from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

def print_response(my_dict):
    desired_order = ['question', 'generated_question', 'answer', 'chat_history', 'source_documents']
    for key in desired_order:
        logger.info(f"\n\n{key}:\n")
        # print(f"    {type(my_dict[key])}")
        if isinstance(my_dict[key], list): 
            for item in my_dict[key]:
                logger.info(f'{item}\n')
        else:
            logger.info(f"    {my_dict[key]}")

#
# def: get_answer  from LLM
#
def get_answer(rag_chain, question):
    response = rag_chain.invoke(question)

    if DEBUG:
        logger.debug(f"Question: {question}")
        logger.debug("The response:")
        logger.debug(response)
        logger.debug()

    return response

def create_rag_chain():
    logger.debug(f'#### ENTER create_rag_chain() function ####')

    # 1. Load a list of documents
    all_docs = load_all_docs()

    # 2. Split docs in chunks
    document_chunks = split_in_chunks(all_docs)

    # 3. Load embeddings model
    embedder = create_embedder()

    # 4. Create a Vectore Store and store embeddings
    vectorstore = create_vectordb(VECTOR_STORE, document_chunks, embedder)

    # 5. Create a retriever
    retriever = create_retriever(vectorstore)

    # 6. Build the LLM
    llm = build_llm()

    # 7. Build prompt template
    template = """You are a helpful assistant who searches for answer by going through provided context below. \
    Answer the question as truthfully as possible using only the context provided. \
    If the answer is not contained within the context below, say "I don't know". \
    Do not end your answer with a question.
    Context: {context}
    Question: {question}
    """

    rag_prompt = ChatPromptTemplate.from_template(template)

    # 8. Build conversation memory
    logger.info("Initializing Conversation Buffer Memory")
    global my_conv_memory
    my_conv_memory = ConversationBufferMemory(
        memory_key="chat_history",
        input_key='question', output_key='answer',
        return_messages=True,
        verbose = True
    )

    # 9. Create a conversation chain
    logger.info("Initializing Conversation Retrieval Chain")
    qa = ConversationalRetrievalChain.from_llm(
        llm, 
        chain_type="stuff", 
        retriever=retriever, 
        memory=my_conv_memory,
        return_source_documents=True,
        return_generated_question=True,
        rephrase_question=False, 
        combine_docs_chain_kwargs={'prompt': rag_prompt}
    )


    logger.info("\nRAG Chain created successfully")

    logger.debug(f'#### EXIT create_rag_chain() function ####')
    return qa

qa = create_rag_chain()

result = qa.invoke("what is machine learning?")
print_response(result)

result = qa.invoke("what are different steps needed to build it?")
print_response(result)

result = qa.invoke("what is oracle autonomous database?")
print_response(result)

result = qa.invoke("what are the different offerings that it provides?")
print_response(result)

